{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 26, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Classifiers and SVM [6 Points]\n",
    "\n",
    "In the lecture [ML-09 Sl.7 and onwards] several types of classifiers have been introduced. The following questions deal with some of them.\n",
    "\n",
    "#### LDA\n",
    "How does the LDA classifier work? What restrictions have to be fullfilled by the data for this method to work and why?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest Neighbor\n",
    "How does the nearest neighbor classifier work? When would you use it and how is it trained?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM's \n",
    "\n",
    "Name some differences between a SVM and a MLP. When would you use which?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Ultimate Dinosaur 3000 M4ze Xtrem!  [14 Points]\n",
    "\n",
    "In this assignment we will have a look at the Q-Learning algorithm described in the lecture [ML-10 Reinforcement Learning]. For this we generate a field with random rewards. A learning agent is then exploring the field and learns the optimal path to navigate through it. The code below is again filled with some ``TODO``'s that should be filled by you in order to implement the Q-Learning algorithm. \n",
    "\n",
    "Below you also find a free-code field for a complete own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "\n",
    "def generate_field(x, y, num_rewards, max_reward):\n",
    "    \"\"\"\n",
    "    Generate a random game field with rewards.\n",
    "    \n",
    "    Args:\n",
    "        x            x dimension of the field\n",
    "        y            y dimension of the field \n",
    "        num_rewards  the number of rewards that should be randomly placed\n",
    "        max_reward   the maximum reward that can be placed \n",
    "        \n",
    "    Returns:\n",
    "        A field with randomly initialized rewards, the rest of the \n",
    "        entries is zero\n",
    "    \"\"\"\n",
    "    field = np.zeros((x,y), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(num_rewards):\n",
    "        field[rand.randint(x), rand.randint(y)] = rand.choice(max_reward)\n",
    "    \n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax algorithm after the forumla: e^x/sum(e^x)\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x) \n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eps_greedy(e):\n",
    "    \"\"\"\n",
    "    Epsilon greedy action selection.\n",
    "    \n",
    "    Args:\n",
    "        e  the probability with which the action is randomly selected\n",
    "    \"\"\"\n",
    "    if np.random.rand > e:\n",
    "        pass\n",
    "        #FIXME finish for sample solution\n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    \"\"\"\n",
    "    This class contains all the necessary methods to navigate through\n",
    "    a maze or game with the help of a little bit of Q-Learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate, map_x, map_y):\n",
    "        \"\"\"\n",
    "        Initializes the QLearning Algorithm with the necessary parameters.\n",
    "        All q values are stored in self.q - this is an array that has\n",
    "        ACTIONS x map_x x map_y dimensions to store a value for each action\n",
    "        in each field. The starting position self.pos is randomly initialized.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate  the gamma in the lecture slides\n",
    "            map_x          x-dimension of the map\n",
    "            map_y          y-dimension of the map\n",
    "        \n",
    "        Returns:\n",
    "            An instance that can be used for QLearning on the field\n",
    "        \"\"\"\n",
    "        # q stores the q_values for each action in each space of the field\n",
    "        self.q = np.zeros((len(ACTIONS), map_x, map_y))\n",
    "        self.gamma = learning_rate\n",
    "        # start on a random position in the field\n",
    "        self.pos = [np.random.randint(map_x), np.random.randint(map_y)]\n",
    "        # remember the map extend for further navigation\n",
    "        self.map_x = map_x\n",
    "        self.map_y = map_y\n",
    "    \n",
    "    def get_coordinates(self, choice):\n",
    "        \"\"\"\n",
    "        Returns the coordinates that follow a certain choice, depending\n",
    "        on the current position of the learner. If the border is reached\n",
    "        the agent just stops there.\n",
    "        \n",
    "        Args:\n",
    "            choice   the action that should be performed (one of: 'up', 'down', ...)\n",
    "            \n",
    "        Returns:\n",
    "            the updated coordinates\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO return the right new coordinates depending on the position\n",
    "        y_new = self.pos[0]\n",
    "        x_new = self.pos[1]\n",
    "        \n",
    "        if   choice == 'up'   : x_new -= 1 if x_new > 0 else 0\n",
    "        elif choice == 'down' : x_new += 1 if x_new < self.map_x - 1 else 0            \n",
    "        elif choice == 'left' : y_new -= 1 if y_new > 0 else 0                \n",
    "        elif choice == 'right': y_new += 1 if y_new < self.map_y - 1 else 0\n",
    "        else: raise ActionError('No such action:', name)\n",
    "            \n",
    "        return (y_new, x_new)\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Implementation of the update step. Closely follows the Algorithm described on\n",
    "        ML-10 Sl.18. Note that the you have attributes available as specified in the\n",
    "        __init__ method of this class, in addition to that is the FIELD variable that\n",
    "        stores the real field the agent is iterating about, as well as ACTIONS which\n",
    "        stores the available actions.\n",
    "        \"\"\"\n",
    "        # TODO:\n",
    "        # get the q-values for the current position of the player\n",
    "        qvals = self.q[:,self.pos[0], self.pos[1]]\n",
    "        \n",
    "        # select next action and exectue it\n",
    "        # dist = softmax(np.asarray(qvals))\n",
    "        \n",
    "        # TODO: select a random action that should be performed next\n",
    "        # be careful to handle the case where you hit the wall!\n",
    "        choice = np.random.choice(ACTIONS, p=dist)\n",
    "        choice_i = ACTIONS.index(choice)\n",
    "        new_pos = self.get_coordinates(choice)\n",
    "        # when we hit the wall step out\n",
    "        if new_pos == self.pos: return self.q\n",
    "             \n",
    "        # TODO: \n",
    "        # receive the reward for the new position\n",
    "        rew = FIELD[new_pos[0], new_pos[1]]\n",
    "        \n",
    "        # TODO:\n",
    "        # update the q-value for the performed action\n",
    "        self.q[choice_i, self.pos[0], self.pos[1]] = rew + self.gamma*max(self.q[:, new_pos[0], new_pos[1]])\n",
    "        \n",
    "        # TODO:\n",
    "        # update the position of the player to the new field\n",
    "        self.pos = new_pos\n",
    "        \n",
    "        return self.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Determine the size of the field, change this parameters as you like\n",
    "m_x = 4\n",
    "m_y = 4\n",
    "\n",
    "steps = 200\n",
    "\n",
    "\n",
    "ACTIONS = ['up','left','right','down']  # those are the availabe actions for the qlearning\n",
    "FIELD = generate_field(m_x, m_y, 5, 10) # the field that is used for learning\n",
    "\n",
    "# Plotting the generated field\n",
    "figure = plt.figure('Field')\n",
    "plt.axis('off')\n",
    "plt.imshow(FIELD, interpolation='none')\n",
    "figure.canvas.draw()\n",
    "\n",
    "# TODO: generate a QLearning instance with the right parameters\n",
    "player = QLearning(0.9, m_x, m_y)\n",
    "\n",
    "fig_player = plt.figure('QLearning State')\n",
    "\n",
    "# now we perform steps many learning iterations on the field with\n",
    "# the generated QLearning instance\n",
    "for i in range(steps):     \n",
    "    player_map = player.update()\n",
    "    \n",
    "    for i, direc in enumerate(ACTIONS):\n",
    "        \n",
    "        plt.subplot(3,3,2*i+2)\n",
    "        plt.axis('off')\n",
    "        plt.title(direc)\n",
    "        plt.imshow(player_map[i,:,:], interpolation = 'None')\n",
    "    \n",
    "    fig_player.canvas.draw()\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are also free to write your complete own implementation of the QLearning algorithm. Use the following cell for your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "maze = np.array([[0, 0, 1], [0, 0, 0]])\n",
    "actions = [lambda c : (c[0], c[1] + 1),\n",
    "           lambda c : (c[0], c[1] - 1),\n",
    "           lambda c : (c[0] - 1, c[1]),\n",
    "           lambda c : (c[0] + 1, c[1])]\n",
    "\n",
    "def move(pos, direction):\n",
    "    new_pos = actions[direction](pos)\n",
    "    for dim, c in enumerate(new_pos):\n",
    "        if c < 0 or c >= maze.shape[dim]:\n",
    "            raise ValueError('Action impossible.')\n",
    "    return new_pos\n",
    "\n",
    "# (Initialize parameters)\n",
    "gamma = 0.9\n",
    "\n",
    "# Initialize q(s, a) <- 0\n",
    "q = np.zeros((np.prod(maze.shape), len(actions)))\n",
    "\n",
    "# Observe current state s\n",
    "position = (0, 0)\n",
    "s = np.ravel_multi_index(position, maze.shape)\n",
    "\n",
    "# Repeat\n",
    "for iteration in range(10000):\n",
    "    # Select action a\n",
    "    a = np.random.randint(len(actions))\n",
    "    # Execute action a (if possible)\n",
    "    try:\n",
    "        position = move(position, a)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # Receive reward r\n",
    "    r = maze[position]\n",
    "    # Observe new state s_n\n",
    "    s_n = np.ravel_multi_index(position, maze.shape)\n",
    "\n",
    "    # Update q(s, a)\n",
    "    q[s, a] = r + gamma * np.max(q[s_n, :])\n",
    "\n",
    "    # Update s\n",
    "    s = s_n\n",
    "\n",
    "print(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osnabr√ºck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet is supposed to last two weeks and thus should be solved and handed in before the end of **Sunday, May 22, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's studip folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip files\n",
    "\n",
    "We provided some zip files for these exercises. Once you downloaded them, just put them alongside this sheet and run the following cell to extract them. This allows us to avoid path problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def extract_zip(filename):\n",
    "    \"\"\"\n",
    "    Extracts a zip file of name filename.zip to the current working directory.\n",
    "    \"\"\"\n",
    "    filename = \"{}.zip\".format(filename)\n",
    "    try:\n",
    "        with open(filename, 'rb') as f:\n",
    "            print(\"Extracting {}...\".format(filename))\n",
    "            z = zipfile.ZipFile(f)\n",
    "            for name in z.namelist():\n",
    "                z.extract(name)\n",
    "            print(\"Extracted {}.\".format(filename))\n",
    "    except FileNotFoundError:\n",
    "        print(\"{} was not found.\".format(filename))\n",
    "\n",
    "\n",
    "for filename in ['eigenfaces', 'leafsnap']:\n",
    "    extract_zip(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciPy, scikit-learn and pillow\n",
    "\n",
    "From now on you will sometimes need the python package [scikit-learn](https://pypi.python.org/pypi/scikit-learn) (scikit-learn) which depends on [scipy](https://pypi.python.org/pypi/scipy). Another package to handle images is [pillow](https://pypi.python.org/pypi/pillow), which we also recommend to use now (matplotlib already has basic image capabilities, but can only deal with `.png` files). To check if you already have running versions of these packages installed, run the following cell. If something is not found, try to follow the installation steps below. Otherwise just skip the following paragraphs and continue with the assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "assert importlib.util.find_spec('scipy') is not None, 'scipy not found'\n",
    "assert importlib.util.find_spec('sklearn') is not None, 'scikit-learn not found'\n",
    "assert importlib.util.find_spec('PIL') is not None, 'pillow not found'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unix\n",
    "\n",
    "On Unix systems you can easily install the packages with `pip3 install scipy scikit-learn pillow` from any terminal window. If it fails, try to figure out how to install a Fortran compiler for your OS or ask one of your fellow tutors for help.\n",
    "\n",
    "#### Windows\n",
    "On Windows it is a little bit more difficult to get a Fortran compiler (and although [MinGW](http://www.mingw.org/) offers one it is still very difficult to get everything to run), so we recommend you to take the [precompiled binaries](http://www.lfd.uci.edu/~gohlke/pythonlibs/) of Christoph Gohlke for [scipy](http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy). If you previously installed a 32bit version of Python download `scipy-0.17.0-cp35-none-win32.whl`, if you have a 64bit version please resort to `scipy-0.17.0-cp35-none-win_amd64.whl`. If you are unsure which version you run, run the following cell to figure it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "print('You are running a {} ({}) version.'.format(*platform.architecture()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the binaries open your command line, navigate to your folder where you downloaded the `*.whl` file to (`cd FOLDER`) and run `pip install scipy-0.17.0-cp35-none-win32.whl` (or `pip install scipy-0.17.0-cp35-none-win_amd64.whl` if you downloaded the 64 bit version)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be a problem with your `numpy` version - `scipy` needs the Intel Math Kernel Library which is not easily compiled manually.\n",
    "If you run into troubles, uninstall `numpy` with `pip uninstall numpy` and download the `*.whl` files from [Gohlke's website](http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy) again. Proceed as you did with scipy.\n",
    "\n",
    "For most other packages `pip install [package]`, e.g. `pip install pillow` should be enough - however, the website mentioned above provides most precompiled binaries and if you run into troubles with the normal installations take a look there.\n",
    "\n",
    "In case of any other problems, get in touch with your tutors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Curse of Dimensionality [6 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following exercise, be detailed in your answers and provide some examples. Think about keywords like: random vectors in high dimensional space, manifolds and Bertillonage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the curse of dimensionality and its implication for pattern classification? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Curse of dimensionality describes the phenomenon that in high dimensional vector spaces, two randomly drawn vectors will almost always be close to orthogonal to each other. This is a real problem in data mining problems, where for a higher number of features, the number of possible combinations and therefore the volume of the resulting feature space exponentionally increases.\n",
    "\n",
    "In such a high dimensional space, data vectors from real data sets lie far away from each other (which means dense sampling becomes impossible, as there aren't enough samples close to each other). This also leads to the problem that pairs of data vectors have a high probability of having similar distances and to be close to orthogonal to each other. The result is that clustering becomes really difficult, as the vectors more or less stand on their own and distance measures cannot be applied easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how this phenomenom could be used to one's advantage."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is actually an advantage if you want to discriminate between a high number of individuals (see Bertillonage, where using only 11 features results in a feature space big enough to discriminate humans), but if you want to get usable information out of data, such a 'singling out' of samples is a great disadvantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain in your own words the concepts of descriptive and intrinsic dimensionality."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Intrinsic dimensionality exists in contrast to the descriptive dimensionality of data, which is defined by the numbers of parameters used to produce or represent the raw data (i.e. the number of pixels in an unprocessed image).\n",
    "\n",
    "Additionally to this representive dimensionality, there is also a (most of the time smaller) number of independent parameters which is necessary to describe the data, always in regard to a specific problem we want to use the data on. \n",
    "For example: a data set might consist of a number of portraits, all with size 1920x1080 pixels, which constitutes their descriptive dimensionality. To do some facial recognition on these portraits however, we do not need the complete descriptive dimension space (which would be way too big anyway), but only a few independent parameters (which we can get by doing PCA and looking at the eigenfaces). \n",
    "This is possible because the data never fill out the entire high dimensional vector space but instead concentrate along a manifold of a much lower dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Implement and Apply PCA [8 Points]\n",
    "\n",
    "In this assignment you will implement PCA from the ground up and apply it to the `cars` dataset (simplified from the JSE [2004 New Car and Truck Data](http://www.amstat.org/publications/jse/jse_data_archive.htm)). This dataset consists of measurements taken on 97 different cars. The eleven features measured are: Suggested retail price (USD), Price to dealer (USD), Engine size (liters), Number of engine cylinders, Engine horsepower, City gas mileage, Highway gas mileage, Weight (pounds), Wheelbase (inches), Length (inches) and Width (inches). \n",
    "\n",
    "We would like to visualize these high dimensional features to get a feeling for how the cars relate to each other so we need to find a subspace of dimension two or three into which we can project the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: Load the cars dataset in cars.csv .\n",
    "cars = np.loadtxt('cars.csv', delimiter=',')\n",
    "\n",
    "assert cars.shape == (97, 11), \"Shape is not (97, 11), was {}\".format(cars.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step we need to normalize the data such that they have a zero mean and a unit standard deviation. Use the standard score for this:\n",
    "$$\\frac{X - \\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: Normalize the data and store it in cars_norm.\n",
    "cars_norm = (cars - np.mean(cars, axis=0)) / np.std(cars, axis=0)\n",
    "\n",
    "assert cars_norm.shape == (97, 11), \"Shape is not (97, 11), was {}\".format(cars.shape)\n",
    "assert np.abs(np.sum(cars_norm)) < 1e-10, \"Absolute sum was {} but should be close to 0\".format(np.abs(np.sum(cars_norm)))\n",
    "assert abs(np.sum(cars_norm ** 2) / cars_norm.size - 1) < 1e-10, \"The data is not normalized, sum/N was {} not 1\".format(np.sum(cars_norm ** 2) / cars_norm.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA finds a subspace that maximizes the variance by determining the eigenvectors of the covariance matrix. So we need to calculate the autocovariance matrix and afterwards the eigenvalues. When the data is normalized the autocovariance is calculated as\n",
    "$$C = X\\cdot X^T$$\n",
    "with $X$ being an $n \\times m$ matrix with $n$ features and $m$ samples.\n",
    "The entry $c_{i,j}$ in $C$ tells you how much feature $i$ correlates with feature $j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: Compute the autocovariance matrix and store it into autocovar\n",
    "autocovar = cars_norm.T @ cars_norm\n",
    "\n",
    "assert autocovar.shape == (11, 11)\n",
    "\n",
    "# TODO: Compute the eigenvalues und eigenvectors and store them into eigenval and eigenvec\n",
    "#       (Figure out a function to do this for you)\n",
    "eigenval, eigenvec = np.linalg.eig(autocovar)\n",
    "\n",
    "assert eigenval.shape == (11,)\n",
    "assert eigenvec.shape == (11, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have a matrix full of eigenvectors. We can now do two things: project the data down onto the two dimensional subspace to visualize it and we can also plot the two first principle component vectors as eleven two dimensional points to get a feeling for how the features are projected into the subspace. Execute the cells below and describe what you see. Is PCA a good method for this problem? Was it justifiable that we only considered the first two principle components? What kinds of cars are in the four quadrants of the first plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project the data down into the two dimensional subspace\n",
    "proj = cars_norm @ eigenvec[:,0:2]\n",
    "\n",
    "# Plot projected data\n",
    "fig = plt.figure('Data projected onto first two Principal Components')\n",
    "fig.gca().set_xlim(-8, 8)\n",
    "fig.gca().set_ylim(-4, 7)\n",
    "plt.scatter(proj[:,0], proj[:,1])\n",
    "# Divide plot into quadrants\n",
    "plt.axhline(0, color='green')\n",
    "plt.axvline(0, color='green')\n",
    "# force drawing on 'run all'\n",
    "fig.canvas.draw()\n",
    "\n",
    "# Plot eigenvectors\n",
    "eig_fig = plt.figure('Eigenvector plot')\n",
    "plt.scatter(eigenvec[:,0], eigenvec[:,1])\n",
    "\n",
    "# add labels\n",
    "labels = ['Suggested retail price (USD)', 'Price to dealer (USD)', \n",
    "          'Engine size (liters)', 'Number of engine cylinders', \n",
    "          'Engine horsepower', 'City gas mileage' , \n",
    "          'Highway gas mileage', 'Weight (pounds)', \n",
    "          'Wheelbase (inches)', 'Length (inches)', 'Width (inches)']\n",
    "for label, x, y in zip(labels, eigenvec[:,0], eigenvec[:,1]):\n",
    "    plt.annotate(\n",
    "        label, xy = (x, y), xytext = (-20, 20),\n",
    "        textcoords = 'offset points', ha = 'left', va = 'bottom',\n",
    "        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'blue', alpha = 0.5),\n",
    "        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "# force drawing on 'run all'\n",
    "eig_fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The first plot shows the complete dataset projected down onto the two first principle components. Only few points overlap and the points are generally spread out well in the subspace. There is not much trend in the plot which is what we desired, i.e. the axis are not redundant. No clusters can be recognized. It is admissible to pick a subspace of dimension two since the eigenvectors have negligible magnitude starting at the third eigenvector. PCA is a good method for this dataset.\n",
    "\n",
    "The second plot shows where a car that has a unit vector as a feature vector would be projected into the subspace. Two custers are recognizable - the gas milage and the other features. This allows us to interpret the first graph better. Cars that are far on the right must have a high gas mileage, either in the city, on the highway or both. They do not have high values on the other categories though. Cars that are high up in the plot are expensive and have high horse power but are rather small and don't weigh much.\n",
    "\n",
    "The quadrants might correspond to: Sports car (top left), family car (bottom left) and limousines or trucks (right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Eigenfaces [6 Points]\n",
    "\n",
    "A very famous example of applying PCA to solve classification problems are Eigenfaces. It employs PCA to project training images of faces into the Eigenspace to reduce the dimensionality of those images by a few magnitudes. To classify test images they are projected into the Eigenspace and the (euclidean) distance between the sample and each reduced training image is calculated - the closest training image wins.\n",
    "\n",
    "To avoid problems with your own implementation of PCA in assignment 2 you will now rely on the `scipy` package which already provides an implementation of PCA.\n",
    "\n",
    "First you will implement the Eigenfaces algorithm and apply it to a subset of the [Extended Yale Face Database B](http://www.cad.zju.edu.cn/home/dengcai/Data/FaceData.html) (the subset of files is stored in `eigenfaces.zip`). Then you will apply the same procedure to another data set, [leafsnap](http://leafsnap.com/dataset/) (`leafsnap.zip`). \n",
    "\n",
    "The face dataset is provided in two sets (`eigenfaces/train` and `eigenfaces/test`) which each yield four images per each of five different people in the [pgm](http://netpbm.sourceforge.net/doc/pgm.html) format (which is supported by `matplotlib` via `pillow`). \n",
    "The leafsnap dataset is provided in two sets as well (`leafsnap/train` and `leafsnap/test`) which each contain two images of leaves from 184 different tree species. They were automatically cropped to square images, resized to a resolution of 200x200 pixels and converted to the [png](https://en.wikipedia.org/wiki/Portable_Network_Graphics) format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Understanding PCA for Eigenfaces\n",
    "\n",
    "To apply PCA on the samples you first need to understand what we are trying to do.\n",
    "\n",
    "Usually a grayscale image is seen as a set of pixels each being a three dimensional datapoint consisting of features X coordinate, Y coordinate and gray value. This is very useful for many types of segmentation or clustering problems, where our image is a space with three dimensions (X, Y, Color).\n",
    "\n",
    "For the Eigenface method we have to consider images in a different way!\n",
    "Here each image is one data point in the hyper space containing all possible images with the same resolution. So each image is a data point consisting of pixel many dimensions or features, each having a value of the color at that pixel.\n",
    "\n",
    "To make this a little bit more clear, consider the 2x2 binary images (see the plot below). Each image is one out of 16 possible data points in the space of all possible 2x2 binary images. The space has four dimensions with two possible values in each, so there are $2^4=16$ possible images. They can be described as feature vectors: \n",
    "$$\\text{image} = \\left(\\matrix{\\text{pixel color top left}\\\\ \\text{pixel color top right}\\\\ \\text{pixel color bottom left}\\\\ \\text{pixel color bottom right}}\\right)$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creates all possible combinations of 4 0s and 1s.\n",
    "images = list(itertools.product([0,1], repeat=4))\n",
    "\n",
    "bin_images = plt.figure('Binary 2x2 Images')\n",
    "for i, img in enumerate(images):\n",
    "    ax = plt.subplot(4, 4, i + 1)\n",
    "    # Remove axis ticks for a better image experience.\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "    # Pretend the artificial arrays are beautiful pictures.\n",
    "    plt.imshow(np.array([img] * 3).T.reshape(2,2,3), interpolation='nearest')\n",
    "# force drawing on 'run all'\n",
    "bin_images.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a relatively small space. What if there were 256 different gray values? The number of possible images increased to $256^4=4,294,967,296$.\n",
    "\n",
    "Our eigenface images are 192x168 pixels each and allow 256 different gray values. This means there are $256^{192 \\cdot 168}=256^{32256}$ possible images (which is a lot as you can see below - a number with $77681$ digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "a = 256 ** (192 * 168)\n",
    "print(\"Digits: {}\".format(math.ceil(math.log(a) / math.log(10))))\n",
    "print(a) # This line takes a second or two to execute, be warned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many principal components are there at most when you apply the PCA with the 20 training face images provided?\n",
    "How many principal components were there for the 16 binary images if we made a PCA on all of them?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are at most 20 principal components possible for the face images, but only four principal components for the binary images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Reading the data\n",
    "\n",
    "Implement the methods `get_sample_database` and `read_sample` in the following cells to easily read in all images we are going to work with. Use the function `get_class_name_from_file` to generate class names to store as `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_class_name_from_file(file):\n",
    "    \"\"\"\n",
    "    Returns a class name from the file path provided.\n",
    "    Takes only the filename (discards the rest of the path), \n",
    "    splits it on underscores and discards the last parts (assumed \n",
    "    to be numbers and file endings). \n",
    "    Then the parts are joined again with spaces and the string is\n",
    "    capitalized.\n",
    "    For example\n",
    "    /Users/esinclair/work/dino_01_1.jpg\n",
    "    will result in\n",
    "    Dino 01\n",
    "    \n",
    "    Args:\n",
    "        file    the filename to operate on\n",
    "    Returns:\n",
    "        A class name derived from the filename.\n",
    "    \"\"\"\n",
    "    return ' '.join(file.split(os.path.sep)[-1][:-4].split('_')[0:2]).capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_sample(imagepath):\n",
    "    \"\"\"\n",
    "    Reads an image file using plt.imread.\n",
    "    If the image has multiple color channels, only the first \n",
    "    channel is returned.\n",
    "    \n",
    "    Args:\n",
    "        imagepath   the path to the image file\n",
    "    Returns:\n",
    "        A two dimensional np array with the color information.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    img = plt.imread(imagepath)\n",
    "    try:\n",
    "        return img[:,:,0]\n",
    "    except IndexError:\n",
    "        return img\n",
    "\n",
    "\n",
    "face = read_sample(os.path.join('eigenfaces', 'test', 'person_01_1.pgm'))\n",
    "leaf = read_sample(os.path.join('leafsnap', 'test', 'acer_palmatum_1.png'))\n",
    "\n",
    "assert face.shape == (192, 168), 'face.shape does not fit! Was: {}'.format(face.shape)\n",
    "assert leaf.shape == (200, 200), 'leaf.shape does not fit! Was: {}'.format(leaf.shape)\n",
    "\n",
    "del face, leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pprint\n",
    "import numpy as np\n",
    "\n",
    "def get_sample_database(path, limit=-1):\n",
    "    \"\"\"\n",
    "    Returns a list of dictionaries {'name': ..., 'data': ...}, where\n",
    "    names correspond the classification class (e.g. person01) and data corresponds\n",
    "    to the image data from that file.\n",
    "    Since there can be multiple training samples for the same classification, the\n",
    "    entries in 'name' don't need to be unique!\n",
    "    \n",
    "    Args:\n",
    "        path    the path to the images (e.g. the eigenfaces/train directory)\n",
    "        limit   the number of images to be read, values < 0 (default is -1) \n",
    "                allow reading of all images\n",
    "    Returns:\n",
    "        A list of dictionaries containing the image names and data, like:\n",
    "        [{'name': 'Person 01', 'data': np.array(...)}, {'name': 'Person 01', 'data': np.array(...)}, ...]\n",
    "    \"\"\"\n",
    "    # Get all sample file paths.\n",
    "    files = glob.glob(os.path.join(path, '*'), recursive=True)\n",
    "\n",
    "    # Reduce the number of samples to the limit.\n",
    "    if limit >= 0:\n",
    "        files = files[0:min(limit, len(files))]\n",
    "\n",
    "    # TODO: Create the database list.\n",
    "    database = []\n",
    "    for file in files:\n",
    "        name = get_class_name_from_file(file)\n",
    "        try:\n",
    "            data = read_sample(file)\n",
    "        except OSError as error:\n",
    "            print('Skipping {} (Error: {})'.format(file, error))\n",
    "            continue\n",
    "        # create database entry with name and data\n",
    "        database += [{'name': name, 'data': data}]\n",
    "    return database\n",
    "\n",
    "\n",
    "faces_test = get_sample_database(os.path.join('eigenfaces', 'test'), 2)\n",
    "leafsnap_test = get_sample_database(os.path.join('leafsnap', 'train'), 1)\n",
    "\n",
    "print('Faces:')\n",
    "pprint.pprint(faces_test)\n",
    "print('Leaves:')\n",
    "pprint.pprint(leafsnap_test)\n",
    "\n",
    "assert len(faces_test) == 2, 'faces_test has length {}'.format(len(faces_test))\n",
    "assert len(leafsnap_test) == 1, 'leafsnap_test has length {}'.format(len(leafsnap_test))\n",
    "\n",
    "assert faces_test[0]['name'] == 'Person 01', \"faces_test[0]['name'] was not Person 01 but {}\".format(face_test[0]['name'])\n",
    "\n",
    "assert isinstance(leafsnap_test[0]['data'], np.ndarray), \"leafsnap_test[0]['data'] is no numpy array\"\n",
    "\n",
    "del faces_test, leafsnap_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Apply PCA\n",
    "\n",
    "To apply the PCA for the Eigenfaces you will implement three more functions. `compose_matrix` and `plot_results` which will be used in addition to the PCA from `sklearn`. Additionally there will be a function which evaluates the success, `report_success`.\n",
    "\n",
    "First implement `compose_matrix`, which creates a raw data matrix out of the databases we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def compose_matrix(database):\n",
    "    \"\"\"\n",
    "    This method creates a raw data matrix from the database provided.\n",
    "    If there are N database entries with an np.array of MxP values as 'data',\n",
    "    then the resulting matrix is of size Nx(M*P).\n",
    "    Thus row 0 contains the pixel values of the 0th element of the database,\n",
    "    row 1 the values of the 1st, etc.\n",
    "    \n",
    "    Args:\n",
    "        database   a database as it is returned by get_sample_database(path, limit)\n",
    "    Returns:\n",
    "        An Nx(M*P) matrix.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return np.stack(db_val['data'].flatten() for db_val in database)\n",
    "\n",
    "\n",
    "# simple example: artificial 2x3 all white and all black images\n",
    "simple_database = [{'name': 'allwhite', 'data': np.array([[1, 1, 1], [1, 1, 1]])}, \n",
    "                   {'name': 'allblack', 'data': np.array([[0, 0, 0], [0, 0, 0]])}]\n",
    "matrix = compose_matrix(simple_database)\n",
    "\n",
    "assert matrix.shape == (2, 6), \"The shape should be (2, 6), was: {}\".format(matrix.shape)\n",
    "assert all(matrix[0,:]), \"The first row should be only 1s, was: {}\".format(matrix[0,:])\n",
    "assert not any(matrix[1,:]), \"The second row should be only 0s, was: {}\".format(matrix[1,:])\n",
    "\n",
    "# complex example: actual images\n",
    "complex_database = get_sample_database(os.path.join('eigenfaces', 'train'), 4)\n",
    "matrix = compose_matrix(complex_database)\n",
    "\n",
    "assert matrix.shape == (4, 32256), \"The shape should be (4, 32256), was: {}\".format(matrix.shape)\n",
    "\n",
    "del simple_database, complex_database, matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next implement `report_success`, a function which returns the success rate and counts for hits and misses. To check if a sample was correctly identified, check the names of the train and test samples for equality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def report_success(train_database, test_database, best_matches):\n",
    "    \"\"\"\n",
    "    This function returns the number of hits and misses.\n",
    "    Takes a train database and a test database as returned by \n",
    "    get_sample_database(path, limit) and a list of best_matches.\n",
    "    \n",
    "    The test database entries are mapped to the train database entries via \n",
    "    the best matches array.\n",
    "    That means for each entry in the test database there has to be a single\n",
    "    integer value in the best matches list which corresponds to the index of\n",
    "    the match for that test sample in the train database.\n",
    "    \n",
    "    Args:\n",
    "        train_database  the training database\n",
    "        test_database   the test database\n",
    "        best_matches    a list of indices (len(best_matches) == len(test_database))\n",
    "                        to map the test samples to their best matching training \n",
    "                        samples\n",
    "    Returns:\n",
    "        Hits, Misses, Ratio\n",
    "            counts for hits and misses, respectively and the ratio hits/total\n",
    "    \"\"\"\n",
    "    hits = sum(train_database[best_matches[index]]['name'] == test_sample['name'] for index, test_sample in enumerate(test_database))\n",
    "    return hits, len(test_database) - hits, hits / len(test_database)\n",
    "\n",
    "\n",
    "train_data = [{'name': 'hit', 'data': None}, \n",
    "              {'name': 'hit', 'data': None},\n",
    "              {'name': 'hit', 'data': None},\n",
    "              {'name': 'success', 'data': None}]\n",
    "test_data  = [{'name': 'hit', 'data': None}, \n",
    "              {'name': 'miss', 'data': None},\n",
    "              {'name': 'hit', 'data': None},\n",
    "              {'name': 'success', 'data': None},\n",
    "              {'name': 'failure', 'data': None}]\n",
    "best_matches = [0, 1, 1, 3, 3]\n",
    "\n",
    "hits, misses, ratio = report_success(train_data, test_data, best_matches)\n",
    "\n",
    "assert hits == 3, \"Result for hits {} is incorrect, should be 3!\".format(hits)\n",
    "assert misses == 2, \"Result for misses {} is incorrect, should be 2!\".format(misses)\n",
    "assert abs(ratio - 0.6) < 1e-10, \"Result for ratio {} is incorrect, should be 0.6!\".format(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement `plot_results`. It should be able to plot at least the first 10 results in one figure (the rest can be skipped), resulting in a subplot with 5 rows and 4 columns, where columns 1 and 3 contain test images and columns 2 and 4 the corresponding result images.\n",
    "\n",
    "*Hint:* Matplotlib's `imshow` will scale (m,n,1) images according to a colormap, thus the results might look a bit different than when you look at the images in your other image programs. To get similar results you can replicate the matrix to shape (m,n,3) - this involves for example `np.tile` and `np.newaxis`, as it can be found on [stackoverflow](http://stackoverflow.com/q/1721802/3004221). (Note that this is not necessary for the exercise, it's enough if you get some images!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(train_database, test_database, best_matches, max_results=float('inf')):\n",
    "    \"\"\"\n",
    "    Takes a train database and a test database as returned by \n",
    "    get_sample_database(path, limit) and a list of best_matches.\n",
    "    \n",
    "    Plots at most max_results many results, where the test database entries\n",
    "    are mapped to the train database entries via the best matches array.\n",
    "    That means for each entry in the test database there has to be a single\n",
    "    integer value in the best matches list which corresponds to the index of\n",
    "    the match for that test sample in the train database.\n",
    "    \n",
    "    The results are plotted in subplots of size 5x4, where the 1st and 3rd \n",
    "    column contain the test images and the 2nd and 4th column the corresponding\n",
    "    train images.\n",
    "    \n",
    "    Args:\n",
    "        train_database  the training database\n",
    "        test_database   the test database\n",
    "        best_matches    a list of indices (len(best_matches) == len(test_database))\n",
    "                        to map the test samples to their best matching training \n",
    "                        samples\n",
    "        max_results     the number of results to plot\n",
    "    \"\"\"\n",
    "    for i, test_sample in enumerate(test_database):\n",
    "        if i >= max_results:\n",
    "            return\n",
    "        if i % 10 == 0:\n",
    "            try:\n",
    "                # force drawing on 'run all'\n",
    "                fig.canvas.draw()\n",
    "            except NameError:\n",
    "                pass\n",
    "            fig = plt.figure()\n",
    "        for j, im in enumerate([test_sample, train_database[best_matches[i]]]):\n",
    "            ax = plt.subplot(5, 4, (2 * i) % 20 + j + 1)\n",
    "            ax.axes.get_xaxis().set_ticks([])\n",
    "            ax.axes.get_yaxis().set_ticks([])\n",
    "            plt.imshow(im['data'])\n",
    "            plt.imshow(np.tile(im['data'][:,:,np.newaxis], (1, 1, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally implement the PCA procedure. Follow the `TODO` comments in the code below (the `TODO (bonus exercise)` is optional but provides some interesting insights towards the understanding of principal components in this method).\n",
    "Figure out how to control the number of components used by the PCA. Find the lowest possible values still able to achieve 100% (90%, 80%) correct classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.decomposition.pca import PCA\n",
    "\n",
    "BASE = 'eigenfaces'\n",
    "# BASE = 'leafsnap'\n",
    "TRAIN = os.path.join(BASE, 'train')\n",
    "TEST = os.path.join(BASE, 'test')\n",
    "\n",
    "# Read in the samples for training and test.\n",
    "train_database = get_sample_database(TRAIN)\n",
    "test_database = get_sample_database(TEST)\n",
    "\n",
    "# TODO: Create data matrices for training and test.\n",
    "raw_traindata = compose_matrix(train_database)\n",
    "raw_testdata = compose_matrix(test_database)\n",
    "\n",
    "# TODO: Create an instance of PCA and fit the training data to it.\n",
    "pca = PCA(5) # 5, 4, 2\n",
    "pca.fit(raw_traindata)\n",
    "\n",
    "# TODO (bonus exercise): Retrieve the principal components from the pca\n",
    "#       and plot them as images. You can limit the number of components\n",
    "#       to plot.\n",
    "resolution = train_database[0]['data'].shape\n",
    "n_pc = min(20, len(pca.components_))\n",
    "princomps = [pc.reshape(resolution) for pc in pca.components_[0:n_pc]]\n",
    "fig = plt.figure('Principal Components {}'.format(BASE))\n",
    "for index, pc in enumerate(princomps):\n",
    "    ax = plt.subplot(5, 4, index + 1)\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "    plt.imshow(pc, cmap='Spectral')\n",
    "\n",
    "# TODO: transform the training data into the new PCA base\n",
    "#       and do the same for the test data.\n",
    "traindata = pca.transform(raw_traindata)\n",
    "testdata = pca.transform(raw_testdata)\n",
    "\n",
    "# TODO: Find the best match for each test sample in the training\n",
    "#       samples. (Consider using cdist).\n",
    "best_matches = np.argmin(cdist(testdata, traindata), 1)\n",
    "\n",
    "# TODO: Report the success and plot the results \n",
    "#       (try only 20 results to not get too many plots).\n",
    "print(\"Hits: {}, Misses: {} ({:.0%})\".format(*report_success(train_database, test_database, best_matches)))\n",
    "plot_results(train_database, test_database, best_matches, 20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PC needed for classification of face images:\n",
    "\n",
    "Classification goal | PC needed \n",
    "--------------------+-----------\n",
    "              100 % | 5\n",
    "               90 % | 4\n",
    "               80 % | 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Eigenleaves\n",
    "\n",
    "In the code above uncomment `# BASE = 'leafsnap'` and run the code again. The calculations will take a little longer as there are more samples involved (remember to only plot a few results). Again try out different numbers of components used by the PCA. Explain the differences of the results between the two datasets."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Eigenfaces only works well under controlled lab scenarios and with heavy manual preprocessing involved. The face images are all pretty similar: eyse, nose, mouth are at similar positions and the lighting is, at least inter-subject-wise, mostly close to constant. The very few variations allow for closely related images.\n",
    "\n",
    "In the leafsnap data however, lab images (training) are often scaled differently than field images (test). The images are often rotated with respect to the lab images. The lighting differes from image to image.\n",
    "\n",
    "Overall the controlled setting and careful preprocessing for the face images are highly important for the success of this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: PCA [4 Points]\n",
    "\n",
    "In this exercise we investigate the statement from the lecture that PCA finds the subspace that captures most of the data variance. To be more precise, we show that the orthonormal projection onto an $m$-dimensional subspace that maximizes the variance of the projected data is defined by the principal components, i.e. by the $m$ eigenvectors of the autocorrelation matrix $C$ corresponding to the $m$ largest eigenvalues. We proceed in two steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "First consider a one dimensional subspace: determine a (unit) vector $\\vec{p}$, such that the variance of the data, when projected onto the subspace determined by that vector, is maximal.\n",
    "\n",
    "The autocorrelation matrix $C$ allows to compute the variance of the projected data as $\\vec{p}^{T}C\\vec{p}$. We want to maximize this expression. To avoid $\\|\\vec{p}\\|\\to\\infty$ we will only consider unit vectors, i.e. we constrain $\\vec{p}$ to be normalized: $\\vec{p}^T\\vec{p}=1$. Maximize the expression with this constraint (which can be done using a Lagrangian multiplier). Conclude that a suitable $\\vec{p}$ has to be an eigenvector of $C$ and describe which of the eigenvectors is optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to maximize the expression\n",
    "$$\\vec{p}^T C\\vec{p} + \\lambda(1-\\vec{p}^T\\vec{p})$$\n",
    "with respect to $\\vec{p}$, i.e. we have to find solutions for\n",
    "$$\\frac{\\partial}{\\partial\\vec{p}}\\left[ \\vec{p}^T C\\vec{p} + \\lambda(1-\\vec{p}^T\\vec{p})\\right] = 0$$\n",
    "This leads to the equation\n",
    "$$C\\vec{p} = \\lambda\\vec{p}$$\n",
    "in other words: for a vector $\\vec{p}$ to maximize our expression, it has to be an eigenvector $C$ and $\\lambda$ has to be the corresponding eigenvalue.\n",
    "By left multiplying with $\\vec{p}^T$ and using the fact that $\\vec{p}^T\\vec{p}=1$, we gain\n",
    "$$\\vec{p}^TC\\vec{p}=\\lambda$$\n",
    "i.e. the projected variance will correspond to the eigenvalue $\\lambda$ and hence is maximized when choosing the largest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "Now proof the statement for the general case of an $m$-dimensional projection space.\n",
    "\n",
    "Use an inductive argument: assume the statement has been shown for the $(m-1)$-dimensional projection space, spanned by the $m-1$ (orthonormal) eigenvectors $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$ corresponding to the $(m-1)$ largest eigenvalues $\\lambda_1,\\ldots,\\lambda_{m-1}$. Now find a (unit) vector $\\vec{p}_m$, orthogonal to the existing vectors $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$, that maximizes the projected variance $\\vec{p}_m^TC\\vec{p}_m$. Proceed similar to case (a), but with additional Lagrangian multipliers to enforce the orthogonality constraint. Show that the new vector $\\vec{p}_m$ is an eigenvector of $C$. Finally show that the variance is maximized for the eigenvector corresponding to the $m$-th largest eigenvalue $\\lambda_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that the result holds for projection spaces of dimensionality $m-1$. We will now show that it then also holds for dimensionality $m$: we consider a subspace spanned by the $m-1$ (orthonormal) eigenvectors $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$ corresponding to the $(m-1)$ largest eigenvalues $\\lambda_1,\\ldots,\\lambda_{m-1}$, and a new vector $\\vec{p}_{m}$ whos properties we will now examine. First, this vector should be linearly independent from $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$, as it should define the new $m$-th dimension. The property can be enforced by the (stronger) requirement that $\\vec{p}_{m}$ should be orthogonal to $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$, i.e. \n",
    "$$\\vec{p}_m^T\\vec{p}_{i}=0 \\text{ for } i=1,\\ldots,m-1,$$\n",
    "which can be expressed using Lagrange multipliers $\\eta_1,\\ldots,\\eta_{m-1}$. As argued in part (a), the variance in direction $\\vec{p}_m$ is given by\n",
    "$$\\vec{p}_{m}^TC\\vec{p}_{m}.$$\n",
    "We want to maximize this value, again with the additional constraint that $\\vec{p}_{m}$ is normalized, i.e.\n",
    "$$\\vec{p}_{m}^T\\vec{p}=1,$$\n",
    "which will be expressed by an additional Lagrange multiplier $\\lambda_M$. So in total we want to maximize the function\n",
    "$$\\vec{p}_{m}^TC\\vec{p}_{m} + \\sum_{i=1}^{m-1}\\eta_i\\vec{p}_m^T\\vec{p}_{i} + \\lambda_m(1-\\vec{p}_{m}^T\\vec{p})$$\n",
    "with respect to $\\vec{p}_m$, i.e. we have to find solutions for\n",
    "\\begin{align}\n",
    "  0\n",
    "  & = \\frac{\\partial}{\\partial\\vec{p}_m}\\left[\\vec{p}_{m}^TC\\vec{p}_{m} \n",
    "  + \\sum_{i=1}^{m-1}\\eta_i\\vec{p}_m^T\\vec{p}_{i}\n",
    "  + \\lambda_m(1-\\vec{p}_{m}^T\\vec{p})\\right] \\\\\n",
    "  & = 2C\\vec{p}_m + \\sum_{i=1}^{m-1}\\eta_i\\vec{p}_m^T\\vec{p}_{i} - 2\\lambda_m\\vec{p}_{m}\n",
    "\\end{align}\n",
    "Multiplying this equation with $\\vec{p}_{j}^T$ from the left yields (due to the orthogonality constraint)\n",
    "\\begin{align}\n",
    "  0 = \\vec{p}_{j}^T 0\n",
    "  & = \\vec{p}_{j}^T 2C\\vec{p}_m +\n",
    "  \\vec{p}_{j}^T \\sum_{i=1}^{m-1}\\eta_i\\vec{p}_m^T\\vec{p}_{i} -\n",
    "  \\vec{p}_{j}^T 2\\lambda_m\\vec{p}_{m} \\\\\n",
    "  &= 0 + \\eta_j\\vec{p}_{j}^T \\vec{p}_{j}- 0 \\\\\n",
    "  & = \\eta_j\n",
    "\\end{align}\n",
    "for $j=1,\\ldots,m-1$. So the problem simplifies to\n",
    "$$0 = 2C\\vec{p}_m - 2\\lambda_m\\vec{p}_{m}$$\n",
    "from which we see that a critical point of the Lagrange equation has to fulfill\n",
    "$$C\\vec{p}_m =\\lambda_m\\vec{p}_{m}$$\n",
    "which just means it has to be an eigenvector of the matrix $C$ with eigenvalue $\\lambda_M$. There may be multiple eigenvectors for $C$, so we have to select $\\vec{p}_m$ in a way that it maximizes the variance in direction $\\vec{p}_m$, i.e. the value\n",
    "$$\\vec{p}_{m}^TC\\vec{p}_{m} = \\vec{p}_{m}^T\\lambda_M\\vec{p}_{m} = \\lambda_M.$$\n",
    "This just means that we have to choose $\\vec{p}_m$ to be the eigenvector with the largest eigenvalue (amongst those not previously selected). This completes the inductive step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
